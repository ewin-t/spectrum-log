@inproceedings{aisw19,
  doi = {10.1109/isit.2019.8849572},
  url = {https://doi.org/10.1109/isit.2019.8849572},
  year = {2019},
  month = jul,
  publisher = {{IEEE}},
  author = {Jayadev Acharya and Ibrahim Issa and Nirmal V. Shende and Aaron B. Wagner},
  title = {Measuring Quantum Entropy},
  booktitle = {2019 {IEEE} International Symposium on Information Theory ({ISIT})}
}

@InProceedings{hjw18,
  title = {Local moment matching: A unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance},
  author = {Han, Yanjun and Jiao, Jiantao and Weissman, Tsachy},
  pages = {3189--3221},
  year = {2018},
  editor = {Sébastien Bubeck and Vianney Perchet and Philippe Rigollet},
  volume = {75},
  series = {Proceedings of Machine Learning Research},
  address = {},
  month = {06--09 Jul},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v75/han18b/han18b.pdf},
  url = {http://proceedings.mlr.press/v75/han18b.html},
  abstract = {We present \emph{Local Moment Matching (LMM)}, a unified methodology for symmetric functional estimation and distribution estimation under Wasserstein distance. We construct an efficiently computable estimator that achieves the minimax rates in estimating the distribution up to permutation, and show that the plug-in approach of our unlabeled distribution estimator is “universal" in estimating symmetric functionals of discrete distributions. Instead of doing best polynomial approximation explicitly as in existing literature of functional estimation, the plug-in approach conducts polynomial approximation implicitly and attains the optimal sample complexity for the entropy, power sum and support size functionals.}
}
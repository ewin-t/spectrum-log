\documentclass{article}
%To instruct arXiv for using pdfLatex (Must be in the first 5 lines to take effect -- no ps will be generated for download.)
\pdfoutput=1

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, mathtools}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{color}
\usepackage{enumitem}
\usepackage{inconsolata}
\usepackage{mleftright}\mleftright
\usepackage{graphicx}
\usepackage{caption}
%\usepackage{fullpage}
%\usepackage{tikz,tikz-cd}
\usepackage{microtype}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{thmtools}
%\usepackage{thm-restate}
\usepackage{url}
\usepackage{verbatim}
\usepackage[linesnumbered,ruled,procnumbered]{algorithm2e}
\usepackage[nameinlink,capitalize]{cleveref}

%\declaretheorem[numberwithin=section]{theorem}
\declaretheorem{theorem}
\declaretheorem[sibling=theorem]{lemma}
\declaretheorem[sibling=theorem]{corollary}
\declaretheorem[sibling=theorem,name=Proposition]{prop}
\theoremstyle{definition}
\declaretheorem[sibling=theorem,name=Problem]{prob}
\declaretheorem[sibling=theorem]{definition}
\declaretheorem[sibling=theorem]{remark}
\declaretheorem[sibling=theorem]{claim}

\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\newcommand{\eps}{\varepsilon}
\newcommand{\ewin}[1]{\textrm{\textcolor[RGB]{0,135,219}{[Ewin: #1]}}}

\begin{document}

Consider probabilities $\alpha_1,\ldots,\alpha_d$.
Note that we'll often think of $\mu$ the discrete distribution that places weight one on each $\alpha_i$.
(This is kind of a different normalization from how it's defined in HJW.)

Our goal will be to use weak Schur sampling to estimate
\begin{align*}
    \int x^k \mu(dx) = \sum_{i=1}^d \alpha_i^k,
\end{align*}
(where bounds on error come from \cite{aisw19}).
Subsequently, we use these estimates to perform the ``local moment matching'' of \cite{hjw18} to hopefully get a better sample complexity.

\section{Classical component}

Trying to interpret the analysis in \cite{hjw18}.

\begin{theorem}
Fix some $K \in \mathbb{N}$.
Suppose that, with probability $\geq 1-\delta$, we have an estimate $\hat{m}_k$ for all $k \in [K]$ such that
\begin{align*}
    \abs[\Big]{\hat{m}_k - \int x^k \mu(dx)} < V_k.
\end{align*}
Then we can find an estimate $\hat{\alpha}$ such that
\begin{align*}
    \E[\|\alpha^< - \hat{\alpha}^<\|_1] \lesssim \frac1K\sqrt{Bd(1+V_1)} + \delta + 2^{9K/2}B\sum B^{-k}V_k
\end{align*}
\end{theorem}

\cite{hjw18} takes $K = c_2 \ln (n)$, $B = c_1\ln(n)/n$, and $V_k = \sqrt{d\ln(n)}(\frac{c_3}{c_1}B)^k = \sqrt{d\ln(n)}(\frac{c_3\ln n}{n})^k$ ($c_3 > c_1$), achieving:
\begin{align*}
    &\E[\|\alpha^< - \hat{\alpha}^<\|_1] \\
    &\lesssim \frac1K\sqrt{Bd(1+V_1)} + \delta + 2^{9K/2}B\sum B^{-k}V_k \\
    &= \frac{1}{c_2\ln(n)}\sqrt{\frac{c_1\ln(n)d}{n}(1+\frac{\sqrt{d}\ln^{1.5}(n)c_3}{n})} + \delta + c_1n^{\frac{9}{2}c_2 - 1}\ln(n)\sum_{k=1}^{c_2\ln(n)} \sqrt{d\ln(n)}\Big(\frac{c_3}{c_1}\Big)^k \\
    &\lesssim \sqrt{\frac{d}{n\ln(n)}(1+\frac{\sqrt{d}\ln^{1.5}(n)c_3}{n})} + \delta + n^{\frac{9}{2}c_2 - 1}\ln(n)\sqrt{d\ln(n)}n^{c_2\ln(c_3/c_1)} \\
    &\lesssim \sqrt{\frac{d}{n\ln(n)}} + \delta + n^{c_4-1}\sqrt{d}
\end{align*}
Where we simply choose parameters to be sufficiently small, and $c_4$ can be as small as needed.
Note that $V_k$ can be quite large: the bound of $\sqrt{d\ln(n)}\cdot (O(B))^k$ can be replaced by any $d^{1-\eps}(O(B))^k$ to get the $\frac{d}{\ln(d)}$ dependence we desire.
(I think the whole argument still goes through even if variance is larger, in fact. It's not an artifact of having this $B$ assumption.)

\begin{proof}
Recall the derivation done in \cite{hjw18}: consider our given probability vector $\alpha = (\alpha_1,\ldots,\alpha_d)$, along with an estimate probability vector $\beta = (\beta_1,\ldots,\beta_d)$ that is formed by considering a measure $\nu$ and discretizing it as in \cite[Definition~8]{hjw18}.
Then
\begin{align*}
    \E[\|\alpha^< - \beta^<\|_1] &= \E[W(\mu, \mu_\beta)] \tag*{by \cite[Lemma~7]{hjw18}} \\
    &= W(\mu, \nu) \tag*{by \cite[Lemma~9]{hjw18}}\\
    &= \sup_{f : \|f\|_{\mathrm{Lip}} \leq 1} \int f(x)(\mu(dx) - \nu(dx)) \tag*{by \cite[Lemma~10]{hjw18}}.
\end{align*}
So, the goal is to find some measure $\nu$ (not necessarily discrete) that cannot be distinguished from the target distribution $\mu$ via 1-Lipschitz functions.

Let $\hat{\mu}$ be any measure satisfying $\hat{\mu}([0,1]) = \mu([0,1]) = n$ and
\begin{align*}
    \abs[\Big]{\hat{m}_k - \int  x^k \hat{\mu}(dx)} < V_k
\end{align*}
for all $k \in [K]$.
From the proof assumption, such a $\hat{\mu}$ exists with probability $\geq 1-\delta$, and by triangle inequality this $\hat{\mu}$ satisfies
\begin{equation} \label{eqn:moments-good}
    \abs*{\int x^k\hat{\mu}(dx) - \int x^k \mu(dx)} < 2V_k.
\end{equation}
This will be our proposed estimate measure.
So, consider some $f: \mathbb{R} \to \mathbb{R}$ that is 1-Lipschitz satisfying $f(0) = 0$ (without loss of generality).
We will take a polynomial approximation to $f$.
Fix a polynomial $P(x) = \sum_{k=0}^K a_kx^k$.
Then
\begin{align*}
    &\abs*{\int f(x)(\mu(dx) - \hat{\mu}(dx))} \\
    & \leq \abs*{\int (f(x) - P(x))(\mu(dx) - \hat{\mu}(dx))} + \abs*{\int P(x)(\mu(dx) - \hat{\mu}(dx))} \\
    & \leq \int \abs{f(x) - P(x)}(\mu(dx) + \hat{\mu}(dx)) + \sum_{k=1}^K \abs{a_k}\cdot 2 V_k \\
    & \leq \int \abs{f(x) - P(x)}(\mu(dx) + \hat{\mu}(dx)) + 2\sum_{k=1}^K \abs{a_k}V_k
\end{align*}

We take $P := \arg\min_Q \max_x |Q(x) - f(x)|$.
Using Jackson's inequality \cite[Lemma~22]{hjw18}, for a constant C,
\begin{align*}
    &\int \abs{f(x) - P(x)}(\mu(dx) - \hat{\mu}(dx)) \\
    &\leq \frac{C\sqrt{B}}{K}\int \sqrt{x}(\mu(dx) + \hat{\mu}(dx)).
\intertext{Continuing with bounding the first term:}
    &\leq \frac{C\sqrt{B}}{K} \sqrt{\Big(\int \sqrt{x}^2 (\mu(dx) + \hat{\mu}(dx))\Big)\Big(\int 1^2 (\mu(dx) + \hat{\mu}(dx))\Big)} \tag*{by Cauchy-Schwarz} \\
    &= \frac{C\sqrt{2Bd}}{K} \sqrt{\int x (\mu(dx) + \hat{\mu}(dx))} \\
    &= \frac{C\sqrt{2Bd}}{K} \sqrt{\int x (2\mu(dx)) + \int x(\hat{\mu}(dx) - \mu(dx))} \\
    &= \frac{C\sqrt{2Bd}}{K} \sqrt{2+2V_1} \tag*{by \cref{eqn:moments-good}} \\
    &\lesssim \frac1K\sqrt{Bd(1+V_1)}
\end{align*}
To upper bound the second part, we need upper bounds on the coefficients.
\begin{align*}
    |P(x)| &\leq |P(x) - f(x)| + |f(x)|
    \leq \frac{CB}{K} + B
\end{align*}
so, using coefficient bounds \cite[Lemma~27]{hjw18}, for all $k \in [K]$
\begin{align*}
    |a_k| &\leq 2^{7K/2 + 1}B\Big(1 + \frac CK\Big)\Big(\frac{B}{2}\Big)^{-k} \\
    &\leq 2^{9K/2 + 1}\Big(1 + \frac CK\Big)B^{1-k}
\end{align*}
and
\begin{align*}
    2\sum_{k=1}^K \abs{a_k}V_k
    &\leq 2\sum_{k=1}^K 2^{9K/2 + 1}\Big(1 + \frac CK\Big)B^{1-k}V_k \\
    &\leq (1+C)2^{9K/2 + 2}\sum B^{1-k}V_k
    \lesssim 2^{9K/2}\sum B^{1-k}V_k
\end{align*}
Putting everything together, we have
\begin{align*}
    &\E \|\alpha^< - \beta^<\|_1 \\
    &=\E \sup_{f : \|f\|_{\text{Lip}} \leq 1} \int_{\mathbb{R}}f(x)(\mu(dx) - \hat{\mu}(dx)) \\
    &\lesssim \Big(\frac1K\sqrt{Bd(1+V_1)} +  2^{9K/2}\sum B^{1-k}V_k\Big) + (\max \|P^< - \hat{P}^<\|_1)(\Pr[\text{alg fails}]) \\
    &\lesssim \frac1K\sqrt{Bd(1+V_1)} +  2^{9K/2}\sum B^{1-k}V_k + \delta \\
\end{align*}
\end{proof}

\section{Quantum component}

\begin{lemma}[{\cite[Lemma~9]{aisw19}}]
    There is a constant $C_k$ depending only on $k$ such that
    \begin{align}
        \E[\frac{1}{n^{\underline{k}}}p_{(k)}^{\#}(\bm{\lambda})] &= \int x^k\mu(dx) \\
        \Var[\frac{1}{n^{\underline{k}}}p_{(k)}^{\#}(\bm{\lambda})] &= C_k\Big(n^{-k} + n^{-1}\int x^{2k-1}\mu(dx)\Big)
    \end{align}
\end{lemma}

Here, $p_{(k)}^\#(\bm{\lambda})$ is the estimator arising from what I think is the naive thing, which is writing the power sum (which they call $M_k(\alpha)$) in the Schur polynomial basis $\{s_\lambda(\alpha)\}$ and using the coefficients to renormalize the probabilities such that the estimator is unbiased. $n^{\underline{k}}$ is the falling power $(n)(n-1)\cdots(n-k+1)$.


\section{Combining the two}
For simplicity, we'll take the case in \cite{hjw18} where $B = O(\ln^2d/d)$.
\begin{align*}
    \E[\|\alpha^< - \hat{\alpha}^<\|_1] &\lesssim \frac1K\sqrt{Bd(1+V_1)} + \delta + 2^{9K/2}B\sum B^{-k}V_k \\
    &\lesssim \frac1K\ln(d)\sqrt{1+V_1} + \delta + \frac{2^{9K/2}}{d}(\poly \log(d))\sum V_kd^k
    \intertext{We'll do a very rough sanity check by judging $V_k$ to be like square root of variance and taking $C_k = 1$ (though this will make a difference in the log factors), making this}
    &\lesssim \frac{\ln d}{K} + \delta + \frac{2^{9K/2}}{d}\poly\log d\sum d^k\sqrt{n^{-k} + n^{-1}d^{3-2k}}
    \intertext{Now, we take $K = O(\ln d)$ for sufficiently small constant.}
    &\lesssim 1 + \delta + d^{\eps-1}\poly\log d\sum d^k\sqrt{n^{-k} + n^{-1}d^{3-2k}} \\
    &\lesssim 1 + \delta + d^{\hat{\eps}-1}\sum d^k\sqrt{n^{-k} + n^{-1}d^{3-2k}}
\end{align*}
Where we bound $V_k^2$ as follows:
\begin{align*}
    n^{-k} + n^{-1}\int x^{2k-1}\mu(dx)
    \leq n^{-k} + n^{-1}dB^{2k-2}
    \lesssim n^{-k} + n^{-1}d^{3-2k}
\end{align*}
The final quantity should be $O(1)$ when we take $n = O(d^2/\poly\log(d))$, which is the desired dependence (presumably one can add dependence on $\eps$ later).

\bibliographystyle{alpha}
\bibliography{main.bib}

\end{document}
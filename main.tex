\documentclass{article}
%To instruct arXiv for using pdfLatex (Must be in the first 5 lines to take effect -- no ps will be generated for download.)
\pdfoutput=1

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, mathtools}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{color}
\usepackage{enumitem}
\usepackage{inconsolata}
\usepackage{mleftright}\mleftright
\usepackage{graphicx}
\usepackage{caption}
%\usepackage{fullpage}
%\usepackage{tikz,tikz-cd}
\usepackage{microtype}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{thmtools}
%\usepackage{thm-restate}
\usepackage{url}
\usepackage{verbatim}
\usepackage[linesnumbered,ruled,procnumbered]{algorithm2e}
\usepackage[nameinlink,capitalize]{cleveref}

%\declaretheorem[numberwithin=section]{theorem}
\declaretheorem{theorem}
\declaretheorem[sibling=theorem]{lemma}
\declaretheorem[sibling=theorem]{corollary}
\declaretheorem[sibling=theorem,name=Proposition]{prop}
\theoremstyle{definition}
\declaretheorem[sibling=theorem,name=Problem]{prob}
\declaretheorem[sibling=theorem]{definition}
\declaretheorem[sibling=theorem]{remark}
\declaretheorem[sibling=theorem]{claim}

\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\newcommand{\eps}{\varepsilon}
\newcommand{\ewin}[1]{\textrm{\textcolor[RGB]{0,135,219}{[Ewin: #1]}}}

\begin{document}

Consider probabilities $\alpha_1,\ldots,\alpha_d$.
Note that we'll often think of $\mu$ the discrete distribution that places weight one on each $\alpha_i$.
(This is kind of a different normalization from how it's defined in HJW.)

Our goal will be to use weak Schur sampling to estimate
\begin{align*}
    \int x^k \mu(dx) = \sum_{i=1}^d \alpha_i^k,
\end{align*}
(where bounds on error come from \cite{aisw19}).
Subsequently, we use these estimates to perform the ``local moment matching'' of \cite{hjw18} to hopefully get a better sample complexity.

\section{Quantum component}

\begin{lemma}[{\cite[Lemma~9]{aisw19}}]
    There is a constant $C_k$ depending only on $k$ such that
    \begin{align}
        \E[p_{(k)}^{\#}(\bm{\lambda})] &= n^{\underline{k}}\int x^k\mu(dx) \\
        \Var[p_{(k)}^{\#}(\bm{\lambda})] &= C_k n^{k}\Big(1 + n^{k - 1}\int x^{2k-1}\mu(dx)\Big)
    \end{align}
\end{lemma}

Here, $p_{(k)}^\#(\bm{\lambda})$ is the estimator arising from what I think is the naive thing, which is writing the power sum (which they call $M_k(\alpha)$) in the Schur polynomial basis $\{s_\lambda(\alpha)\}$ and using the coefficients to renormalize the probabilities such that the estimator is unbiased. $n^{\underline{k}}$ is the falling power $(n)(n-1)\cdots(n-k+1)$.

\section{Classical component}

Trying to interpret the analysis in \cite{hjw18}.

\begin{theorem}
Fix some $K \in \mathbb{N}$ \ewin{needs optimization, maybe?}.
Suppose that, with probability $\geq 1-\delta$, we have an estimate $\hat{m}_k$ for all $k \in [K]$ such that
\begin{align*}
    \abs[\Big]{\hat{m}_k - \int x^k \mu(dx)} < VB^k.
\end{align*}
Then we can find a $\hat{\alpha}$ such that
\begin{align*}
    \E[\|\alpha^< - \hat{\alpha}^<\|_1] \lesssim \frac{\sqrt{Bd}}{K} + 2^{9K/2}VB + \delta
\end{align*}
\end{theorem}

\begin{proof}
Recall the derivation done in \cite{hjw18}: consider our given probability vector $\alpha = (\alpha_1,\ldots,\alpha_d)$, along with an estimate probability vector $\beta = (\beta_1,\ldots,\beta_d)$ that is formed by considering a measure $\nu$ and discretizing it as in \cite[Definition~8]{hjw18}.
Then
\begin{align*}
    \frac1d\E[\|\alpha^< - \beta^<\|_1] = \E[W(\mu, \mu_\beta)] = W(\mu, \nu) = \sup_{f : \|f\|_{\mathrm{Lip}} \leq 1} \int f(x)(\mu(dx) - \nu(dx)).
\end{align*}
So, the goal is to find some measure $\nu$ (not necessarily discrete) that cannot be distinguished from the target distribution $\mu$ via 1-Lipschitz functions.

Let $\hat{\mu}$ be any measure satisfying $\hat{\mu}([0,1]) = \mu([0,1]) = n$ and
\begin{align*}
    \abs[\Big]{\hat{m}_k - \int  x^k \hat{\mu}(dx)} < VB^k
\end{align*}
for all $k \in [K]$.
From the proof assumption, such a $\hat{\mu}$ exists with probability $\geq 1-\delta$, and by triangle inequality this $\hat{\mu}$ satisfies
\begin{equation} \label{eqn:moments-good}
    \abs*{\int x^k\hat{\mu}(dx) - \int x^k \mu(dx)} < 2VB^k.
\end{equation}
This will be our proposed estimate measure.
So, consider some $f: \mathbb{R} \to \mathbb{R}$ that is 1-Lipschitz satisfying $f(0) = 0$ (without loss of generality).
We will take a polynomial approximation to $f$.
Fix a polynomial $P(x) = \sum_{k=0}^K a_kx^k$.
Then
\begin{align*}
    &\abs*{\int f(x)(\mu(dx) - \hat{\mu}(dx))} \\
    & \leq \abs*{\int (f(x) - P(x))(\mu(dx) - \hat{\mu}(dx))} + \abs*{\int P(x)(\mu(dx) - \hat{\mu}(dx))} \\
    & \leq \int \abs{f(x) - P(x)}(\mu(dx) + \hat{\mu}(dx)) + \sum_{k=1}^K \abs{a_k}\cdot 2 VB^k \\
    & \leq \int \abs{f(x) - P(x)}(\mu(dx) + \hat{\mu}(dx)) + 2V \sum_{k=1}^K \abs{a_k}B^k
\end{align*}

We take $P := \arg\min_Q \max_x |Q(x) - f(x)|$.
Using Jackson's inequality (Lemma 22), for a constant C,
\begin{align*}
    &\int \abs{f(x) - P(x)}(\mu(dx) - \hat{\mu}(dx)) \\
    &\leq \frac{C\sqrt{B}}{K}\int \sqrt{x}(\mu(dx) + \hat{\mu}(dx)).
\intertext{Continuing with bounding the first term:}
    &\leq \frac{C\sqrt{B}}{K} \sqrt{\Big(\int \sqrt{x}^2 (\mu(dx) + \hat{\mu}(dx))\Big)\Big(\int 1^2 (\mu(dx) + \hat{\mu}(dx))\Big)} \tag*{by Cauchy-Schwarz} \\
    &= \frac{C\sqrt{2Bd}}{K} \sqrt{\int x (\mu(dx) + \hat{\mu}(dx))} \\
    &= \frac{C\sqrt{2Bd}}{K} \sqrt{\int x (2\mu(dx)) + \int x(\hat{\mu}(dx) - \mu(dx))} \\
    &= \frac{C\sqrt{2Bd}}{K} \sqrt{2+2BV} \tag*{by \cref{eqn:moments-good}} \\
    &\lesssim \frac1K\sqrt{Bd+B^2Vd}
\end{align*}
To upper bound the second part, we need upper bounds on the coefficients.
\begin{align*}
    |P(x)| &\leq |P(x) - f(x)| + |f(x)|
    \leq \frac{CB}{K} + B
\end{align*}
so, using coefficient bounds (Lemma 27), for all $k \in [K]$
\begin{align*}
    |a_k| &\leq 2^{7K/2 + 1}B\Big(1 + \frac CK\Big)\Big(\frac{B}{2}\Big)^{-k} \\
    &= 2^{9K/2 + 1}\Big(1 + \frac CK\Big)B^{1-k}
\end{align*}
and
\begin{align*}
    2V\sum_{k=1}^K \abs{a_k}B^k
    &\leq 2V\sum_{k=1}^K 2^{9K/2 + 1}\Big(1 + \frac CK\Big)B^{1-k}B^k \\
    &\leq 2V(1+C)2^{9K/2 + 2}B
\end{align*}
Putting everything together, we have
\begin{align*}
    &\E \|\alpha^< - \beta^<\|_1 \\
    &=\E \sup_{f : \|f\|_{\text{Lip}} \leq 1} \int_{\mathbb{R}}f(x)(\mu(dx) - \hat{\mu}(dx)) \\
    &\lesssim \Big(\frac1K\sqrt{Bd+B^2Vd} + V2^{9K/2}B\Big) + (\max \|P^< - \hat{P}^<\|_1)(\Pr[\text{alg fails}]) \\
    \intertext{Taking $K = c\ln n$ for sufficiently small $c$ and $B = O(\frac{\ln n}{n})$:}
    &\lesssim \sqrt{\frac{d}{n\ln n}} + n^{\eps-1}V + 2\delta.
\end{align*}
(Assuming that $VB = O(1)$.)
So, if we are in the same parameter setting as HJW, we'd want to take $V = d^{1-O(\eps)}$ (HJW gets $V = \sqrt{d}$).
\end{proof}

\bibliographystyle{alpha}
\bibliography{main.bib}

\end{document}